{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following code uses Keras to build CNN model, in combination of training and testing data augmentation and pesudo labeling.\n",
    "\n",
    "- The code is partially adapted from https://www.kaggle.com/devm2024/keras-model-for-beginners-0-210-on-lb-eda-r-d\n",
    "\n",
    "\n",
    "- The features we use are band_1, band_2 and (band_1+band_2)/2. Inc_angle is not used and no additional feature engineering are used while I'm pretty sure the result will benefit from further engineering the features. This and other information can be found from: https://earth.esa.int/c/document_library/get_file?folderId=409229&name=DLFE-5566.pdf (Credit to Jian Wang)\n",
    "\n",
    "\n",
    "- The input is a 75x75x3 set of images. The output is a probability bwtween 0/1 where 1 is noted as an iceberg.\n",
    "\n",
    "\n",
    "- The code is written in the following order:\n",
    "    - Data engineering and image normalization\n",
    "    - data augmentation for engineered training data (X_train can contain multiple images)\n",
    "    - data augmentation for engineered testing data (X_test also can contain multiple images)\n",
    "    - pesudo labeling\n",
    "    \n",
    "\n",
    "- This code will generate the following results in .csv format:\n",
    "    - prediction result without data augmentation\n",
    "    - prediction result with only training data augmentation\n",
    "    - prediction result with both training and testing data augmentation\n",
    "    - prediction result with pesudo labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from os.path import join as opj\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import pylab\n",
    "plt.rcParams['figure.figsize'] = 10, 10\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_json('train.json')\n",
    "df_test = pd.read_json('test.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data engineering and image normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define image normalization function\n",
    "\n",
    "def get_scaled_imgs(df):\n",
    "    imgs = []\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        #make 75x75 image\n",
    "        band_1 = np.array(row['band_1']).reshape(75, 75)\n",
    "        band_2 = np.array(row['band_2']).reshape(75, 75)\n",
    "        band_3 = (band_1 + band_2)/2 # plus since log(x*y) = log(x) + log(y)\n",
    "        \n",
    "        # Rescale\n",
    "        a = (band_1 - band_1.mean()) / (band_1.max() - band_1.min())\n",
    "        b = (band_2 - band_2.mean()) / (band_2.max() - band_2.min())\n",
    "        c = (band_3 - band_3.mean()) / (band_3.max() - band_3.min())\n",
    "\n",
    "        imgs.append(np.dstack((a, b, c)))\n",
    "\n",
    "    return np.array(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get X_train, y_train\n",
    "X_train = get_scaled_imgs(df_train)\n",
    "y_train = df_train['is_iceberg']\n",
    "\n",
    "# get X_test\n",
    "X_test = get_scaled_imgs(df_test)\n",
    "\n",
    "# split data into training set and validation set\n",
    "X_train_cv, X_valid, y_train_cv, y_valid = train_test_split(X_train, y_train, random_state = 16, train_size = 0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Import Keras and define CNN model\n",
    "Note: There are still lots of room to tune the CNN model to optimize the prediction results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Import Keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Input, Flatten, Activation\n",
    "from keras.layers import GlobalMaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.models import Model\n",
    "from keras import initializers\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define CNN model. \n",
    "\n",
    "The dimension of the input is (75, 75, 3) which corresponds to band_1, band_2 and (band_1 + band_2)/2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define CNN model\n",
    "def getModel():\n",
    "    #Building the model\n",
    "    gmodel=Sequential()\n",
    "    #Conv Layer 1\n",
    "    gmodel.add(Conv2D(32, kernel_size=(3, 3),activation='relu', input_shape=(75, 75, 3)))  # change input dimension\n",
    "    gmodel.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
    "    gmodel.add(Dropout(0.2))\n",
    "\n",
    "    #Conv Layer 2\n",
    "    gmodel.add(Conv2D(64, kernel_size=(3, 3), activation='relu' ))\n",
    "    gmodel.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "    gmodel.add(Dropout(0.2))\n",
    "\n",
    "    #Conv Layer 3\n",
    "    gmodel.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "    gmodel.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "    gmodel.add(Dropout(0.2))\n",
    "\n",
    "    #Conv Layer 4\n",
    "    gmodel.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "    gmodel.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "    gmodel.add(Dropout(0.2))\n",
    "\n",
    "    #Flatten the data for upcoming dense layers\n",
    "    gmodel.add(Flatten())\n",
    "\n",
    "    #Dense Layers\n",
    "    gmodel.add(Dense(256)) # can be doubled\n",
    "    gmodel.add(Activation('relu'))\n",
    "    gmodel.add(Dropout(0.2))\n",
    "\n",
    "    #Dense Layer 2\n",
    "    gmodel.add(Dense(128)) # can be doubled\n",
    "    gmodel.add(Activation('relu'))\n",
    "    gmodel.add(Dropout(0.2))\n",
    "\n",
    "    #Sigmoid Layer\n",
    "    gmodel.add(Dense(1))\n",
    "    gmodel.add(Activation('sigmoid'))\n",
    "\n",
    "    mypotim = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    gmodel.compile(loss='binary_crossentropy',\n",
    "                  optimizer=mypotim,\n",
    "                  metrics=['accuracy'])\n",
    "    gmodel.summary()\n",
    "    return gmodel\n",
    "\n",
    "# define filepath and callbacks\n",
    "def get_callbacks(filepath, patience):\n",
    "    es = EarlyStopping('val_loss', patience = patience, mode = \"min\")\n",
    "    msave = ModelCheckpoint(filepath, save_best_only = True)\n",
    "    return [es, msave]\n",
    "file_path = \".model_weights.hdf5\"\n",
    "callbacks = get_callbacks(filepath = file_path, patience = 5)\n",
    "\n",
    "# define parameters\n",
    "batch_size = 128\n",
    "epochs = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Result without data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 73, 73, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 36, 36, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 36, 36, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 34, 34, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 17, 17, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 17, 17, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 15, 15, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 5, 5, 128)         147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 2, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 2, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 129       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 405,185\n",
      "Trainable params: 405,185\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "1203/1203 [==============================] - 22s 19ms/step - loss: 0.7225 - acc: 0.5835\n",
      "Epoch 2/30\n",
      "1203/1203 [==============================] - 19s 16ms/step - loss: 0.5914 - acc: 0.6451\n",
      "Epoch 3/30\n",
      "1203/1203 [==============================] - 19s 16ms/step - loss: 0.4714 - acc: 0.8080\n",
      "Epoch 4/30\n",
      "1203/1203 [==============================] - 19s 15ms/step - loss: 0.3691 - acc: 0.8446\n",
      "Epoch 5/30\n",
      "1203/1203 [==============================] - 21s 18ms/step - loss: 0.3343 - acc: 0.8495\n",
      "Epoch 6/30\n",
      "1203/1203 [==============================] - 20s 16ms/step - loss: 0.3201 - acc: 0.8504\n",
      "Epoch 7/30\n",
      "1203/1203 [==============================] - 20s 17ms/step - loss: 0.2983 - acc: 0.8612\n",
      "Epoch 8/30\n",
      "1203/1203 [==============================] - 20s 17ms/step - loss: 0.2646 - acc: 0.8761\n",
      "Epoch 9/30\n",
      "1203/1203 [==============================] - 21s 17ms/step - loss: 0.2550 - acc: 0.8853\n",
      "Epoch 10/30\n",
      "1203/1203 [==============================] - 22s 18ms/step - loss: 0.2637 - acc: 0.8761\n",
      "Epoch 11/30\n",
      "1203/1203 [==============================] - 23s 19ms/step - loss: 0.2567 - acc: 0.8795\n",
      "Epoch 12/30\n",
      "1203/1203 [==============================] - 19s 16ms/step - loss: 0.2526 - acc: 0.8828\n",
      "Epoch 13/30\n",
      "1203/1203 [==============================] - 19s 16ms/step - loss: 0.2552 - acc: 0.8811\n",
      "Epoch 14/30\n",
      "1203/1203 [==============================] - 20s 17ms/step - loss: 0.2266 - acc: 0.8986\n",
      "Epoch 15/30\n",
      "1203/1203 [==============================] - 19s 16ms/step - loss: 0.2278 - acc: 0.8978\n",
      "Epoch 16/30\n",
      "1203/1203 [==============================] - 19s 16ms/step - loss: 0.2345 - acc: 0.9011\n",
      "Epoch 17/30\n",
      "1203/1203 [==============================] - 20s 16ms/step - loss: 0.2290 - acc: 0.8911\n",
      "Epoch 18/30\n",
      "1203/1203 [==============================] - 19s 16ms/step - loss: 0.2470 - acc: 0.8878\n",
      "Epoch 19/30\n",
      "1203/1203 [==============================] - 19s 16ms/step - loss: 0.2047 - acc: 0.9069\n",
      "Epoch 20/30\n",
      "1203/1203 [==============================] - 19s 16ms/step - loss: 0.1931 - acc: 0.9144\n",
      "Epoch 21/30\n",
      "1203/1203 [==============================] - 20s 17ms/step - loss: 0.1624 - acc: 0.9277\n",
      "Epoch 22/30\n",
      "1203/1203 [==============================] - 20s 17ms/step - loss: 0.1572 - acc: 0.9327\n",
      "Epoch 23/30\n",
      "1203/1203 [==============================] - 19s 16ms/step - loss: 0.1752 - acc: 0.9285\n",
      "Epoch 24/30\n",
      "1203/1203 [==============================] - 22s 18ms/step - loss: 0.1425 - acc: 0.9435\n",
      "Epoch 25/30\n",
      "1203/1203 [==============================] - 20s 16ms/step - loss: 0.1452 - acc: 0.9335\n",
      "Epoch 26/30\n",
      "1203/1203 [==============================] - 20s 16ms/step - loss: 0.1279 - acc: 0.9443\n",
      "Epoch 27/30\n",
      "1203/1203 [==============================] - 20s 16ms/step - loss: 0.1361 - acc: 0.9476\n",
      "Epoch 28/30\n",
      "1203/1203 [==============================] - 21s 18ms/step - loss: 0.1559 - acc: 0.9310\n",
      "Epoch 29/30\n",
      "1203/1203 [==============================] - 20s 17ms/step - loss: 0.1344 - acc: 0.9426\n",
      "Epoch 30/30\n",
      "1203/1203 [==============================] - 20s 17ms/step - loss: 0.1170 - acc: 0.9534\n",
      "401/401 [==============================] - 2s 6ms/step\n",
      "Result without data augmentation\n",
      "Validation Loss:  0.256786955804\n",
      "Validation Accuracy:  0.915211970075\n"
     ]
    }
   ],
   "source": [
    "# train and test validation set without data augmentation\n",
    "gmodel = getModel()\n",
    "\n",
    "gmodel.load_weights(filepath = '.model_weights.hdf5')\n",
    "\n",
    "gmodel.fit(X_train_cv, y_train_cv, epochs=epochs, batch_size=batch_size, verbose = 1, validation_split = 0) \n",
    "\n",
    "loss, acc = gmodel.evaluate(X_valid, y_valid, batch_size = batch_size, verbose = 1)\n",
    "print('Result without data augmentation')\n",
    "print('Validation Loss: ', loss)\n",
    "print('Validation Accuracy: ', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train data augmentation using Keras\n",
    "Here we use Keras ImageDataGenerator for the data augmentation. There are many parameters to tune and after spending time optimizing the parameters, we noticed that shift(including width and height), rotation, zoom and flip of the images can improve the performance of the model, while shear, samplewise_center and samplewise_std_normalization will significantly decrease the prediction accuracy. Channel shift has negligible influence on the model performance. ZCA whitening can also decrease the performance of the model. The parameters we use below is merely one set of parameters on which we achieved the best results among all those investigated. Further efforts can be put into optimizing the data augmentation parameters in order to improve the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data augmentation using Keras\n",
    "datagen = ImageDataGenerator(zoom_range = 0.2,\n",
    "                         horizontal_flip = True,\n",
    "                         vertical_flip = True\n",
    "                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"manually\" train the training dataset. Here the batch_size is 128 and the epochs number is 30*4 = 120."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_9 (Conv2D)            (None, 73, 73, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 36, 36, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 36, 36, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 34, 34, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 17, 17, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 17, 17, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 15, 15, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 5, 5, 128)         147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 2, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 2, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 129       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 405,185\n",
      "Trainable params: 405,185\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 0\n",
      "Training...\n",
      "1203/1203 [==============================] - 28s 23ms/step - train loss: 0.6918 - train acc: 0.5295\n",
      "Epoch 1\n",
      "Training...\n",
      "1203/1203 [==============================] - 22s 18ms/step - train loss: 0.6650 - train acc: 0.5802\n",
      "Epoch 2\n",
      "Training...\n",
      "1203/1203 [==============================] - 25s 21ms/step - train loss: 0.6150 - train acc: 0.6517\n",
      "Epoch 3\n",
      "Training...\n",
      "1203/1203 [==============================] - 22s 18ms/step - train loss: 0.5559 - train acc: 0.7174\n",
      "Epoch 4\n",
      "Training...\n",
      "1203/1203 [==============================] - 23s 19ms/step - train loss: 0.5274 - train acc: 0.7456\n",
      "Epoch 5\n",
      "Training...\n",
      "1203/1203 [==============================] - 22s 18ms/step - train loss: 0.4684 - train acc: 0.7789\n",
      "Epoch 6\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 16ms/step - train loss: 0.4245 - train acc: 0.8163\n",
      "Epoch 7\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 17ms/step - train loss: 0.3898 - train acc: 0.8238\n",
      "Epoch 8\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 17ms/step - train loss: 0.3785 - train acc: 0.8146\n",
      "Epoch 9\n",
      "Training...\n",
      "1203/1203 [==============================] - 22s 18ms/step - train loss: 0.4309 - train acc: 0.8013\n",
      "Epoch 10\n",
      "Training...\n",
      "1203/1203 [==============================] - 21s 17ms/step - train loss: 0.4078 - train acc: 0.8204\n",
      "Epoch 11\n",
      "Training...\n",
      "1203/1203 [==============================] - 21s 17ms/step - train loss: 0.3528 - train acc: 0.8437\n",
      "Epoch 12\n",
      "Training...\n",
      "1203/1203 [==============================] - 22s 18ms/step - train loss: 0.3615 - train acc: 0.8263\n",
      "Epoch 13\n",
      "Training...\n",
      "1203/1203 [==============================] - 23s 19ms/step - train loss: 0.3413 - train acc: 0.8304\n",
      "Epoch 14\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 17ms/step - train loss: 0.3225 - train acc: 0.8554\n",
      "Epoch 15\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 17ms/step - train loss: 0.3022 - train acc: 0.8462\n",
      "Epoch 16\n",
      "Training...\n",
      "1203/1203 [==============================] - 21s 17ms/step - train loss: 0.3150 - train acc: 0.8579\n",
      "Epoch 17\n",
      "Training...\n",
      "1203/1203 [==============================] - 21s 18ms/step - train loss: 0.3496 - train acc: 0.8346\n",
      "Epoch 18\n",
      "Training...\n",
      "1203/1203 [==============================] - 22s 18ms/step - train loss: 0.3394 - train acc: 0.8446\n",
      "Epoch 19\n",
      "Training...\n",
      "1203/1203 [==============================] - 21s 18ms/step - train loss: 0.3296 - train acc: 0.8454\n",
      "Epoch 20\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 16ms/step - train loss: 0.2981 - train acc: 0.8537\n",
      "Epoch 21\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 17ms/step - train loss: 0.2949 - train acc: 0.8587\n",
      "Epoch 22\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 17ms/step - train loss: 0.2964 - train acc: 0.8637\n",
      "Epoch 23\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 17ms/step - train loss: 0.2863 - train acc: 0.8670\n",
      "Epoch 24\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 16ms/step - train loss: 0.2867 - train acc: 0.8662\n",
      "Epoch 25\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 17ms/step - train loss: 0.2833 - train acc: 0.8637\n",
      "Epoch 26\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 17ms/step - train loss: 0.2891 - train acc: 0.8695\n",
      "Epoch 27\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 17ms/step - train loss: 0.2826 - train acc: 0.8612\n",
      "Epoch 28\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 17ms/step - train loss: 0.3010 - train acc: 0.8579\n",
      "Epoch 29\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 17ms/step - train loss: 0.2731 - train acc: 0.8761\n",
      "Epoch 30\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 17ms/step - train loss: 0.2869 - train acc: 0.8695\n",
      "Epoch 31\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 16ms/step - train loss: 0.2619 - train acc: 0.8861\n",
      "Epoch 32\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 16ms/step - train loss: 0.2711 - train acc: 0.8836\n",
      "Epoch 33\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 16ms/step - train loss: 0.2817 - train acc: 0.8728\n",
      "Epoch 34\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 17ms/step - train loss: 0.2515 - train acc: 0.8786\n",
      "Epoch 35\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 17ms/step - train loss: 0.2503 - train acc: 0.8919\n",
      "Epoch 36\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 17ms/step - train loss: 0.2492 - train acc: 0.8944\n",
      "Epoch 37\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 17ms/step - train loss: 0.2485 - train acc: 0.8828\n",
      "Epoch 38\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 16ms/step - train loss: 0.2383 - train acc: 0.8961\n",
      "Epoch 39\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 17ms/step - train loss: 0.2847 - train acc: 0.8836\n",
      "Epoch 40\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 17ms/step - train loss: 0.2396 - train acc: 0.8978\n",
      "Epoch 41\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 16ms/step - train loss: 0.2446 - train acc: 0.8878\n",
      "Epoch 42\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 16ms/step - train loss: 0.2246 - train acc: 0.9019\n",
      "Epoch 43\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 16ms/step - train loss: 0.2326 - train acc: 0.8936\n",
      "Epoch 44\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 16ms/step - train loss: 0.2325 - train acc: 0.8994\n",
      "Epoch 45\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 16ms/step - train loss: 0.2357 - train acc: 0.9036\n",
      "Epoch 46\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 17ms/step - train loss: 0.2256 - train acc: 0.9002\n",
      "Epoch 47\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 16ms/step - train loss: 0.2524 - train acc: 0.8778\n",
      "Epoch 48\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 16ms/step - train loss: 0.2864 - train acc: 0.8828\n",
      "Epoch 49\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 16ms/step - train loss: 0.2637 - train acc: 0.8845\n",
      "Epoch 50\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 17ms/step - train loss: 0.2427 - train acc: 0.8928\n",
      "Epoch 51\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 16ms/step - train loss: 0.2352 - train acc: 0.8978\n",
      "Epoch 52\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 16ms/step - train loss: 0.2183 - train acc: 0.8978\n",
      "Epoch 53\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 16ms/step - train loss: 0.2197 - train acc: 0.9069\n",
      "Epoch 54\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 17ms/step - train loss: 0.2323 - train acc: 0.8994\n",
      "Epoch 55\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 17ms/step - train loss: 0.2274 - train acc: 0.9052\n",
      "Epoch 56\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 17ms/step - train loss: 0.2497 - train acc: 0.8820\n",
      "Epoch 57\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 16ms/step - train loss: 0.2474 - train acc: 0.8961\n",
      "Epoch 58\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 16ms/step - train loss: 0.2066 - train acc: 0.9061\n",
      "Epoch 59\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 17ms/step - train loss: 0.2174 - train acc: 0.9036\n",
      "Epoch 60\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 16ms/step - train loss: 0.2592 - train acc: 0.8894\n",
      "Epoch 61\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 17ms/step - train loss: 0.2608 - train acc: 0.8961\n",
      "Epoch 62\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 16ms/step - train loss: 0.2280 - train acc: 0.9044\n",
      "Epoch 63\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 17ms/step - train loss: 0.2057 - train acc: 0.9152\n",
      "Epoch 64\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 16ms/step - train loss: 0.2075 - train acc: 0.9127\n",
      "Epoch 65\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 17ms/step - train loss: 0.2209 - train acc: 0.9086\n",
      "Epoch 66\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 16ms/step - train loss: 0.2095 - train acc: 0.9077\n",
      "Epoch 67\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 16ms/step - train loss: 0.2018 - train acc: 0.9152\n",
      "Epoch 68\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 16ms/step - train loss: 0.2067 - train acc: 0.9160\n",
      "Epoch 69\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 16ms/step - train loss: 0.1875 - train acc: 0.9177\n",
      "Epoch 70\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 17ms/step - train loss: 0.2118 - train acc: 0.9160\n",
      "Epoch 71\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 16ms/step - train loss: 0.2091 - train acc: 0.9086\n",
      "Epoch 72\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 16ms/step - train loss: 0.1931 - train acc: 0.9227\n",
      "Epoch 73\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 16ms/step - train loss: 0.1895 - train acc: 0.9177\n",
      "Epoch 74\n",
      "Training...\n",
      "1203/1203 [==============================] - 21s 17ms/step - train loss: 0.2345 - train acc: 0.8953\n",
      "Epoch 75\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 17ms/step - train loss: 0.2015 - train acc: 0.9144\n",
      "Epoch 76\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 16ms/step - train loss: 0.2018 - train acc: 0.9169\n",
      "Epoch 77\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 16ms/step - train loss: 0.1892 - train acc: 0.9185\n",
      "Epoch 78\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 16ms/step - train loss: 0.2170 - train acc: 0.9044\n",
      "Epoch 79\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 16ms/step - train loss: 0.1945 - train acc: 0.9227\n",
      "Epoch 80\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 16ms/step - train loss: 0.1837 - train acc: 0.9244\n",
      "Epoch 81\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 16ms/step - train loss: 0.1903 - train acc: 0.9169\n",
      "Epoch 82\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 16ms/step - train loss: 0.1786 - train acc: 0.9335\n",
      "Epoch 83\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 17ms/step - train loss: 0.2227 - train acc: 0.8953\n",
      "Epoch 84\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 16ms/step - train loss: 0.1870 - train acc: 0.9210\n",
      "Epoch 85\n",
      "Training...\n",
      "1203/1203 [==============================] - 22s 19ms/step - train loss: 0.1820 - train acc: 0.9227\n",
      "Epoch 86\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 16ms/step - train loss: 0.1931 - train acc: 0.9185\n",
      "Epoch 87\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 16ms/step - train loss: 0.1890 - train acc: 0.9135\n",
      "Epoch 88\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 16ms/step - train loss: 0.1807 - train acc: 0.9260\n",
      "Epoch 89\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 16ms/step - train loss: 0.1852 - train acc: 0.9227\n",
      "Epoch 90\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 16ms/step - train loss: 0.1712 - train acc: 0.9244\n",
      "Epoch 91\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 16ms/step - train loss: 0.1851 - train acc: 0.9177\n",
      "Epoch 92\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 17ms/step - train loss: 0.1841 - train acc: 0.9260\n",
      "Epoch 93\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 16ms/step - train loss: 0.1789 - train acc: 0.9293\n",
      "Epoch 94\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 17ms/step - train loss: 0.1798 - train acc: 0.9235\n",
      "Epoch 95\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 16ms/step - train loss: 0.1565 - train acc: 0.9393\n",
      "Epoch 96\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 16ms/step - train loss: 0.1688 - train acc: 0.9352\n",
      "Epoch 97\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 16ms/step - train loss: 0.1632 - train acc: 0.9343\n",
      "Epoch 98\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 16ms/step - train loss: 0.1630 - train acc: 0.9368\n",
      "Epoch 99\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 16ms/step - train loss: 0.1586 - train acc: 0.9318\n",
      "Epoch 100\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 16ms/step - train loss: 0.1633 - train acc: 0.9318\n",
      "Epoch 101\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 17ms/step - train loss: 0.1644 - train acc: 0.9277\n",
      "Epoch 102\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 17ms/step - train loss: 0.1581 - train acc: 0.9268\n",
      "Epoch 103\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 17ms/step - train loss: 0.1575 - train acc: 0.9393\n",
      "Epoch 104\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 17ms/step - train loss: 0.1599 - train acc: 0.9385\n",
      "Epoch 105\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 17ms/step - train loss: 0.1700 - train acc: 0.9252\n",
      "Epoch 106\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 16ms/step - train loss: 0.1401 - train acc: 0.9393\n",
      "Epoch 107\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 16ms/step - train loss: 0.1670 - train acc: 0.9302\n",
      "Epoch 108\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 16ms/step - train loss: 0.1397 - train acc: 0.9393\n",
      "Epoch 109\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 17ms/step - train loss: 0.1559 - train acc: 0.9335\n",
      "Epoch 110\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 16ms/step - train loss: 0.1647 - train acc: 0.9302\n",
      "Epoch 111\n",
      "Training...\n",
      "1203/1203 [==============================] - 21s 17ms/step - train loss: 0.1609 - train acc: 0.9377\n",
      "Epoch 112\n",
      "Training...\n",
      "1203/1203 [==============================] - 21s 17ms/step - train loss: 0.1670 - train acc: 0.9160\n",
      "Epoch 113\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 17ms/step - train loss: 0.1492 - train acc: 0.9410\n",
      "Epoch 114\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 17ms/step - train loss: 0.1499 - train acc: 0.9451\n",
      "Epoch 115\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 16ms/step - train loss: 0.1475 - train acc: 0.9435\n",
      "Epoch 116\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 16ms/step - train loss: 0.1337 - train acc: 0.9485\n",
      "Epoch 117\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 16ms/step - train loss: 0.1393 - train acc: 0.9460\n",
      "Epoch 118\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 17ms/step - train loss: 0.1430 - train acc: 0.9393\n",
      "Epoch 119\n",
      "Training...\n",
      "1203/1203 [==============================] - 20s 16ms/step - train loss: 0.1575 - train acc: 0.9277\n"
     ]
    }
   ],
   "source": [
    "# \"manually\" train the training dataset \n",
    "from keras.utils import generic_utils\n",
    "\n",
    "gmodel = getModel()\n",
    "for e in range(epochs*4):\n",
    "    print('Epoch', e)\n",
    "    print('Training...')\n",
    "    progbar = generic_utils.Progbar(X_train_cv.shape[0])\n",
    "    batches = 0\n",
    "\n",
    "    for x_batch, y_batch in datagen.flow(X_train_cv, y_train_cv, batch_size = batch_size, shuffle = True):\n",
    "        loss,train_acc = gmodel.train_on_batch(x_batch, y_batch)\n",
    "        batches += x_batch.shape[0]\n",
    "        if batches > X_train_cv.shape[0]:\n",
    "            break\n",
    "        progbar.add(x_batch.shape[0], values=[('train loss', loss),('train acc', train_acc)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "401/401 [==============================] - 2s 5ms/step\n",
      "Result with data augmentation\n",
      "Validation Loss:  0.197882395303\n",
      "Validation Accuracy:  0.922693266833\n"
     ]
    }
   ],
   "source": [
    "# train and test validation set with train data augmentation\n",
    "loss, acc = gmodel.evaluate(X_valid, y_valid, batch_size=batch_size, verbose = 1)\n",
    "print('Result with data augmentation')\n",
    "print('Validation Loss: ', loss)\n",
    "print('Validation Accuracy: ', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8424/8424 [==============================] - 44s 5ms/step\n"
     ]
    }
   ],
   "source": [
    "# predict test without test data augmentation\n",
    "predicted_test = gmodel.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame()\n",
    "submission['id'] = df_test['id']\n",
    "submission['is_iceberg'] = predicted_test.reshape((predicted_test.shape[0]))\n",
    "submission.to_csv('result_train_aug.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test data augmentation\n",
    "Here we try to make sure the test dataset is augmented the same way as the train dataset. The predicted result of each entry is the average value of results from its multiple augmented images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8424/8424 [==============================] - 42s 5ms/step\n",
      "8424/8424 [==============================] - 41s 5ms/step\n",
      "8424/8424 [==============================] - 41s 5ms/step\n"
     ]
    }
   ],
   "source": [
    "test_datagen_0 = ImageDataGenerator(zoom_range = 0.2) \n",
    "test_datagen_1 = ImageDataGenerator(horizontal_flip = True) \n",
    "test_datagen_2 = ImageDataGenerator(vertical_flip = True) \n",
    "\n",
    "testbatch_size = 1\n",
    "\n",
    "predicted_test = np.asarray([0.0]*8424,dtype = np.float32) # the test data has 8424 entries.\n",
    "predicted_test = predicted_test.reshape(predicted_test.shape + (1,))\n",
    "num_para = 3\n",
    "j = 0\n",
    "\n",
    "while j < num_para:\n",
    "    X_test_aug = []\n",
    "    for i in range(len(X_test)):\n",
    "        k = 0\n",
    "        X_test_reshape = X_test[i].reshape((1,) + X_test[i].shape) \n",
    "        i += 1\n",
    "        for batch in eval('test_datagen_{}.flow(X_test_reshape, batch_size = testbatch_size, seed = 0)'.format(j)):      \n",
    "            X_test_aug.append(batch.squeeze(axis = 0))\n",
    "            k += 1\n",
    "            if k >= testbatch_size:\n",
    "                break\n",
    "    j += 1 \n",
    "    X_test_aug = np.asarray(X_test_aug, dtype=np.float32)\n",
    "    result = gmodel.predict_proba(X_test_aug)\n",
    "    \n",
    "    # take the average\n",
    "    predicted_test += np.asarray(result, dtype = np.float32)/(num_para*1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame()\n",
    "submission['id'] = df_test['id']\n",
    "submission['is_iceberg'] = predicted_test.reshape((predicted_test.shape[0]))\n",
    "submission.to_csv('result_train_test_aug.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Pseudo labeling\n",
    "Pseudo labeling is a semi-supervised machine learning method used when it's hard to get enough training dataset. The general procedure is 1) Use the model to predict the testing dataset 2) use the result as label to retrain the combination of both the training and testing dataset 3) Use the retrained model with psuedo labeling data to predict the test sample. Please refer to the following link for details:\n",
    "https://www.analyticsvidhya.com/blog/2017/09/pseudo-labelling-semi-supervised-learning-technique/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Use the model to predict the testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8424/8424 [==============================] - 43s 5ms/step\n"
     ]
    }
   ],
   "source": [
    "pred_test_classes = gmodel.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Retrain the combination of both the training and testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_total = np.concatenate((X_test,X_train),axis = 0)\n",
    "y_train_total = np.concatenate((pred_test_classes,y_train[:,np.newaxis]),axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "10028/10028 [==============================] - 168s 17ms/step - loss: 0.1956 - acc: 0.9181\n",
      "Epoch 2/50\n",
      "10028/10028 [==============================] - 165s 16ms/step - loss: 0.1709 - acc: 0.9305\n",
      "Epoch 3/50\n",
      "10028/10028 [==============================] - 168s 17ms/step - loss: 0.1625 - acc: 0.9317\n",
      "Epoch 4/50\n",
      "10028/10028 [==============================] - 156s 16ms/step - loss: 0.1508 - acc: 0.9389\n",
      "Epoch 5/50\n",
      "10028/10028 [==============================] - 156s 16ms/step - loss: 0.1398 - acc: 0.9419\n",
      "Epoch 6/50\n",
      "10028/10028 [==============================] - 158s 16ms/step - loss: 0.1317 - acc: 0.9457\n",
      "Epoch 7/50\n",
      "10028/10028 [==============================] - 158s 16ms/step - loss: 0.1317 - acc: 0.9455\n",
      "Epoch 8/50\n",
      "10028/10028 [==============================] - 162s 16ms/step - loss: 0.1286 - acc: 0.9453\n",
      "Epoch 9/50\n",
      "10028/10028 [==============================] - 167s 17ms/step - loss: 0.1124 - acc: 0.9568\n",
      "Epoch 10/50\n",
      "10028/10028 [==============================] - 160s 16ms/step - loss: 0.1134 - acc: 0.9548\n",
      "Epoch 11/50\n",
      "10028/10028 [==============================] - 156s 16ms/step - loss: 0.1022 - acc: 0.9598\n",
      "Epoch 12/50\n",
      "10028/10028 [==============================] - 157s 16ms/step - loss: 0.0982 - acc: 0.9612\n",
      "Epoch 13/50\n",
      "10028/10028 [==============================] - 154s 15ms/step - loss: 0.0976 - acc: 0.9606\n",
      "Epoch 14/50\n",
      "10028/10028 [==============================] - 154s 15ms/step - loss: 0.0896 - acc: 0.9630\n",
      "Epoch 15/50\n",
      "10028/10028 [==============================] - 153s 15ms/step - loss: 0.0856 - acc: 0.9663\n",
      "Epoch 16/50\n",
      "10028/10028 [==============================] - 158s 16ms/step - loss: 0.0894 - acc: 0.9643\n",
      "Epoch 17/50\n",
      "10028/10028 [==============================] - 159s 16ms/step - loss: 0.0901 - acc: 0.9659\n",
      "Epoch 18/50\n",
      "10028/10028 [==============================] - 159s 16ms/step - loss: 0.0835 - acc: 0.9670\n",
      "Epoch 19/50\n",
      "10028/10028 [==============================] - 163s 16ms/step - loss: 0.0732 - acc: 0.9713\n",
      "Epoch 20/50\n",
      "10028/10028 [==============================] - 160s 16ms/step - loss: 0.0733 - acc: 0.9714\n",
      "Epoch 21/50\n",
      "10028/10028 [==============================] - 164s 16ms/step - loss: 0.0697 - acc: 0.9742\n",
      "Epoch 22/50\n",
      "10028/10028 [==============================] - 154s 15ms/step - loss: 0.0723 - acc: 0.9711\n",
      "Epoch 23/50\n",
      "10028/10028 [==============================] - 158s 16ms/step - loss: 0.0598 - acc: 0.9776\n",
      "Epoch 24/50\n",
      "10028/10028 [==============================] - 159s 16ms/step - loss: 0.0605 - acc: 0.9781\n",
      "Epoch 25/50\n",
      "10028/10028 [==============================] - 160s 16ms/step - loss: 0.0596 - acc: 0.9748\n",
      "Epoch 26/50\n",
      "10028/10028 [==============================] - 169s 17ms/step - loss: 0.0561 - acc: 0.9785\n",
      "Epoch 27/50\n",
      "10028/10028 [==============================] - 163s 16ms/step - loss: 0.0592 - acc: 0.9783\n",
      "Epoch 28/50\n",
      "10028/10028 [==============================] - 167s 17ms/step - loss: 0.0538 - acc: 0.9810\n",
      "Epoch 29/50\n",
      "10028/10028 [==============================] - 156s 16ms/step - loss: 0.0512 - acc: 0.9808\n",
      "Epoch 30/50\n",
      "10028/10028 [==============================] - 156s 16ms/step - loss: 0.0534 - acc: 0.9790\n",
      "Epoch 31/50\n",
      "10028/10028 [==============================] - 156s 16ms/step - loss: 0.0453 - acc: 0.9826\n",
      "Epoch 32/50\n",
      "10028/10028 [==============================] - 155s 15ms/step - loss: 0.0501 - acc: 0.9816\n",
      "Epoch 33/50\n",
      "10028/10028 [==============================] - 155s 16ms/step - loss: 0.0471 - acc: 0.9821\n",
      "Epoch 34/50\n",
      "10028/10028 [==============================] - 155s 15ms/step - loss: 0.0527 - acc: 0.9807\n",
      "Epoch 35/50\n",
      "10028/10028 [==============================] - 156s 16ms/step - loss: 0.0471 - acc: 0.9821\n",
      "Epoch 36/50\n",
      "10028/10028 [==============================] - 157s 16ms/step - loss: 0.0449 - acc: 0.9829\n",
      "Epoch 37/50\n",
      "10028/10028 [==============================] - 156s 16ms/step - loss: 0.0495 - acc: 0.9812\n",
      "Epoch 38/50\n",
      "10028/10028 [==============================] - 156s 16ms/step - loss: 0.0408 - acc: 0.9862\n",
      "Epoch 39/50\n",
      "10028/10028 [==============================] - 156s 16ms/step - loss: 0.0434 - acc: 0.9842\n",
      "Epoch 40/50\n",
      "10028/10028 [==============================] - 155s 15ms/step - loss: 0.0402 - acc: 0.9854\n",
      "Epoch 41/50\n",
      "10028/10028 [==============================] - 156s 16ms/step - loss: 0.0327 - acc: 0.9887\n",
      "Epoch 42/50\n",
      "10028/10028 [==============================] - 156s 16ms/step - loss: 0.0382 - acc: 0.9855\n",
      "Epoch 43/50\n",
      "10028/10028 [==============================] - 156s 16ms/step - loss: 0.0360 - acc: 0.9869\n",
      "Epoch 44/50\n",
      "10028/10028 [==============================] - 38685s 4s/step - loss: 0.0354 - acc: 0.9867\n",
      "Epoch 45/50\n",
      "10028/10028 [==============================] - 162s 16ms/step - loss: 0.0494 - acc: 0.9826\n",
      "Epoch 46/50\n",
      "10028/10028 [==============================] - 160s 16ms/step - loss: 0.0385 - acc: 0.9862\n",
      "Epoch 47/50\n",
      "10028/10028 [==============================] - 165s 16ms/step - loss: 0.0277 - acc: 0.9898\n",
      "Epoch 48/50\n",
      "10028/10028 [==============================] - 157s 16ms/step - loss: 0.0397 - acc: 0.9849\n",
      "Epoch 49/50\n",
      "10028/10028 [==============================] - 156s 16ms/step - loss: 0.0342 - acc: 0.9883\n",
      "Epoch 50/50\n",
      "10028/10028 [==============================] - 156s 16ms/step - loss: 0.0306 - acc: 0.9880\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26026ec0240>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmodel.fit(X_train_total, y_train_total, batch_size=batch_size, epochs=50, verbose=1, validation_split = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Use the retrained model with psuedo labeling data to predict the test sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10028/10028 [==============================] - 49s 5ms/step\n",
      "Result with pseudo labeling\n",
      "Validation Loss:  0.0019593254536\n",
      "Validation Accuracy:  0.999900279218\n"
     ]
    }
   ],
   "source": [
    "loss, acc = gmodel.evaluate(X_train_total, y_train_total, batch_size=batch_size, verbose = 1)\n",
    "print('Result with pseudo labeling')\n",
    "print('Validation Loss: ', loss)\n",
    "print('Validation Accuracy: ', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8424/8424 [==============================] - 44s 5ms/step\n"
     ]
    }
   ],
   "source": [
    "predicted_test = gmodel.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame()\n",
    "submission['id'] = df_test['id']\n",
    "submission['is_iceberg'] = predicted_test.reshape((predicted_test.shape[0]))\n",
    "submission.to_csv('result_pseudo_labeling.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Data normalization significantly improves the accuracy of the prediction results (log loss decrease from ~0.21 to ~0.19).\n",
    "2. Both train data augmentation and test data augmentation improves the accuracy of the prediction results (log loss decrease ~ 0.01).\n",
    "3. Pesudo-labeling is tricky here. \n",
    "   - By definition, we can tell that it can push the prediction results to extremes, either much better or much less. \n",
    "   - In this case, the training dataset is only ~ 25% of the testing dataset. It is quite likely that the prediction error\n",
    "     on the testing dataset is propagated into the second round of training and further boosted. Therefore, the log loss      increased to as high as ~ 0.78. \n",
    "   - One key parameter to tune here is retrain the augmented training dataset and testing dataset, where much more data\n",
    "are included and overfitting can probably better avoided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
